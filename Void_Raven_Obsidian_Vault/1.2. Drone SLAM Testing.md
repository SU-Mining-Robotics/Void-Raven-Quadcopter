

# 1. Pose and Velocity Estimation

1. [Optical Flow](https://docs.px4.io/main/en/sensor/optical_flow.html) provides 2D velocity estimation (using a downward facing camera and a downward facing distance sensor)
2. [Visual Inertial Odometry (VIO)](https://docs.px4.io/main/en/computer_vision/visual_inertial_odometry.html) provides 3D pose and velocity estimation using an onboard vision system and IMU. It is used for navigation when global position information is absent or unreliable.

# 2. Collision Avoidance + Path Planning

1. [Collision Prevention](https://docs.px4.io/main/en/computer_vision/collision_prevention.html) is used to stop vehicles before they can crash into an obstacle (primarily when flying in manual modes).

# 3. SLAM with D435i
## Overview 
1. Check out [this video](https://www.youtube.com/watch?v=tcJHnHpwCXk&ab_channel=IntelRealSense) where the source code can bee found [here](https://github.com/IntelRealSense/realsense-ros/wiki/SLAM-with-D435i) 
	1. Problem: This is for ROS1 not ROS2. Can this be converted?
2. Only Problem:
	1. Doesn't do path planning and collision avoidance

## Testing RGBD SLAM (RTabMap, Set Cam IMU, Trying IMU integration for position)

1. Use SLAM with D435i section and get slam to work
	1. used [this link](https://github.com/simonbogh/realsense-d435-rtab-map-in-ROS2/tree/main) but use humble, not foxy when installing dependencies
	2. Launch using:
In first Tab launch realsense camera:
```Shell
ros2 launch realsense2_camera rs_launch.py align_depth.enable:=true pointcloud.enable:=true depth_module.depth_profile:=848x480x30
```
or(imu, gyro, accel)
```Shell
ros2 launch realsense2_camera rs_launch.py enable_accel:=true enable_gyro:=true unite_imu_method:=1
```
OR (include imu, gyro, accel)
```Shell
ros2 launch realsense2_camera rs_launch.py align_depth.enable:=true pointcloud.enable:=true enable_accel:=true enable_gyro:=true unite_imu_method:=2
```
1=copy, 2=interpolation

In a second terminal launch RTabMap:
```Shell
ros2 launch rtabmap_launch rtabmap.launch.py \
 args:="--delete_db_on_start" \
 depth_topic:=/camera/camera/aligned_depth_to_color/image_raw \
 rgb_topic:=/camera/camera/color/image_raw \
 camera_info_topic:=/camera/camera/color/camera_info \
 approx_sync:=false \
 frame_id:=camera_link
```
OR
```Shell
ros2 launch rtabmap_launch rtabmap.launch.py \
 rtabmap_args:="--delete_db_on_start" \
 rgb_topic:=/camera/camera/color/image_raw \
 depth_topic:=/camera/camera/aligned_depth_to_color/image_raw \
 camera_info_topic:=/camera/camera/color/camera_info \
 frame_id:=camera_link
 approx_sync:=true \
 wait_imu_to_init:=true \
 imu_topic:=/camera/camera/imu
```
/imu/data
To see imu in RVIZ2, install:
```Shell
sudo apt-get install ros-humble-imu-tools
```
and run
```shell
ros2 run imu_filter_madgwick imu_filter_madgwick_node --ros-args -p use_mag:=false -r /imu/data_raw:=/camera/camera/imu
```

Calibrate Camera using [this link](https://www.intelrealsense.com/wp-content/uploads/2019/07/Intel_RealSense_Depth_D435i_IMU_Calibration.pdf) 

Run the created node that converts acceleration to position
```shell
ros2 run imu_position_pkg imu_position_node
```


Next: 
1. acceleration/movement in rviz?
	1. Accelerometer provides acceleration, write code to find v and distance, can be  use in RVIZ
2. add movement to point cloud to build map



# 4.  SLAM with OAK-D cameras

1. Check out [this link](https://docs.luxonis.com/software/ros/vio-slam#VIO%20and%20SLAM-RAE%20on-device%20VIO%20%26%20SLAM) 
2. Also Check out this [RTABMAP on Pi4](https://www.instructables.com/RGB-D-SLAM-With-Kinect-on-Raspberry-Pi-4-Buster-RO/) , but running on ROS1
3. 



# 99. Strategy for implementation:
## DONE
1. Use SLAM with D435i section and get slam to work
	1. used [this link](https://github.com/simonbogh/realsense-d435-rtab-map-in-ROS2/tree/main) but use humble, not foxy when installing dependencies
	2. Works, but is very laggy. doubt that  it will work on drone RBPi


## ToDo

	3. ToDo:
		1. Check if SLAM with D435i can be converted to ros2
		2. Try Orbslam3 with stereo-inertial/depth inertial
		3. Try other VIO SLAM for Ros2. Maybe Kimera or similar
3. Check if this can work with collision avoidance from PX4
4. Find a way to perform path planning and deploy aswell
5. 

























# Visual Based vs Lidar SLAM (Reword)

Visual SLAM relies on cameras to capture images of the environment, which are
then processed using computer vision algorithms to extract features. Laser SLAM
uses LiDAR sensors to capture 3D point clouds of the environment and estimate the
robot position and orientation.

![[Pasted image 20240801084818.png]]

Therefore decided on visual.


# Different SLAM Algorithms

![[Pasted image 20240801085448.png]]

Therefore mainly look at:
1. RTab_Map
	- https://github.com/introlab/rtabmap
	- 
1. ORB_SLAM3
	- https://github.com/UZ-SLAMLab/ORB_SLAM3 ("We have tested the library in **Ubuntu 16.04** and **18.04**, but it should be easy to compile in other platforms. A powerful computer (e.g. i7 **(written in 2021)** ) will ensure real-time performance and provide more stable and accurate results"). - **Might still be worth a try**
	- https://github.com/suchetanrs/ORB-SLAM3-ROS2-Docker
	- 
1. Elastic Fusion("A very fast nVidia GPU 3.5TFLOPS+, and a fast CPU (something like an i7 **(written in 2016)** "). -Doubt it, but possibly worth a try
	- https://github.com/FengyuGuo/ElasticFusionRos
	- 
Then look at:
1. RGBD-SLAM-v2 (apparently not for ARM and needs GPU to "Detected keypoints using SIFT (GPU)")
	-  https://github.com/felixendres/rgbdslam_v2
	- 
1. OKVIS
	- https://github.com/ethz-asl/okvis
	- 
1. DVO
	- https://github.com/tum-vision/dvo
	- 














